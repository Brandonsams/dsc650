{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries and define common helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import errno\n",
    "import sys\n",
    "import gzip\n",
    "import json\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import pyarrow as pa\n",
    "from pyarrow.json import read_json\n",
    "import pyarrow.parquet as pq\n",
    "import fastavro\n",
    "import pygeohash\n",
    "import snappy\n",
    "import jsonschema\n",
    "from jsonschema.exceptions import ValidationError\n",
    "import math\n",
    "\n",
    "\n",
    "endpoint_url='https://storage.budsc.midwest-datascience.com'\n",
    "\n",
    "current_dir = Path(os.getcwd()).absolute()\n",
    "schema_dir = current_dir.joinpath('schemas')\n",
    "results_dir = current_dir.joinpath('results')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def read_jsonl_data():\n",
    "    s3 = s3fs.S3FileSystem(\n",
    "        anon=True,\n",
    "        client_kwargs={\n",
    "            'endpoint_url': endpoint_url\n",
    "        }\n",
    "    )\n",
    "    src_data_path = 'data/processed/openflights/routes.jsonl.gz'\n",
    "    with s3.open(src_data_path, 'rb') as f_gz:\n",
    "        with gzip.open(f_gz, 'rb') as f:\n",
    "            records = [json.loads(line) for line in f.readlines()]\n",
    "        \n",
    "\n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the records from https://storage.budsc.midwest-datascience.com/data/processed/openflights/routes.jsonl.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = read_jsonl_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.a JSON Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def validate_jsonl_data(records):\n",
    "    schema_path = schema_dir.joinpath('routes-schema.json')\n",
    "    validation_csv_path = results_dir.joinpath('validation-results.csv')\n",
    "    with open(schema_path) as f:\n",
    "        schema = json.load(f)\n",
    "    with open(validation_csv_path, 'w') as f:\n",
    "        fieldnames = ['row_num', 'is_valid', 'msg']\n",
    "        csv_writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        csv_writer.writeheader()\n",
    "        for i, record in enumerate(records):\n",
    "            try:\n",
    "                ## TODO\n",
    "                # The JSON conforms to the schema\n",
    "                jsonschema.validate(record, schema=schema)\n",
    "                result = dict(\n",
    "                    ## TODO\n",
    "                    row_num = i,\n",
    "                    is_valid = True,\n",
    "                    msg = record\n",
    "                )\n",
    "            except ValidationError as e:\n",
    "                # The JSON does not conform to the schema\n",
    "                result = dict(\n",
    "                    ## TODO\n",
    "                    row_num = i,\n",
    "                    is_valid = False,\n",
    "                    msg = record\n",
    "                )\n",
    "                #print(record)\n",
    "            finally:\n",
    "                csv_writer.writerow(result)\n",
    "validate_jsonl_data(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.b Avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_avro_dataset(records):\n",
    "    schema_path = schema_dir.joinpath('routes.avsc')\n",
    "    data_path = results_dir.joinpath('routes.avro')\n",
    "    \n",
    "    ## TODO: Use fastavro to create Avro dataset\n",
    "    with open(schema_path) as f:\n",
    "        # Make sure that schema is correct\n",
    "        schema = json.load(f)\n",
    "    with open(data_path, 'wb') as out:\n",
    "        fastavro.writer(out, schema, records)\n",
    "    \n",
    "        \n",
    "create_avro_dataset(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.c Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parquet_dataset():\n",
    "    src_data_path = 'data/processed/openflights/routes.jsonl.gz'\n",
    "    parquet_output_path = results_dir.joinpath('routes.parquet')\n",
    "    s3 = s3fs.S3FileSystem(\n",
    "        anon=True,\n",
    "        client_kwargs={\n",
    "            'endpoint_url': endpoint_url\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    with s3.open(src_data_path, 'rb') as f_gz:\n",
    "        with gzip.open(f_gz, 'rb') as f:\n",
    "            ## TODO: Use Apache Arrow to create Parquet table and save the dataset\n",
    "            table = pa.json.read_json(f)\n",
    "            pq.write_table(table, parquet_output_path)#, row_group_size=1)        \n",
    "\n",
    "create_parquet_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.d Protocol Buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.abspath('routes_pb2'))\n",
    "\n",
    "import routes_pb2\n",
    "\n",
    "def _airport_to_proto_obj(airport):\n",
    "    obj = routes_pb2.Airport()\n",
    "    if airport is None:\n",
    "        return None\n",
    "        # return obj\n",
    "    if airport.get('airport_id') is None:\n",
    "        return None\n",
    "        # return obj\n",
    "\n",
    "    obj.airport_id = airport.get('airport_id')\n",
    "    if airport.get('name'):\n",
    "        obj.name = airport.get('name')\n",
    "    if airport.get('city'):\n",
    "        obj.city = airport.get('city')\n",
    "    if airport.get('iata'):\n",
    "        obj.iata = airport.get('iata')\n",
    "    if airport.get('icao'):\n",
    "        obj.icao = airport.get('icao')\n",
    "    if airport.get('altitude'):\n",
    "        obj.altitude = airport.get('altitude')\n",
    "    if airport.get('timezone'):\n",
    "        obj.timezone = airport.get('timezone')\n",
    "    if airport.get('dst'):\n",
    "        obj.dst = airport.get('dst')\n",
    "    if airport.get('tz_id'):\n",
    "        obj.tz_id = airport.get('tz_id')\n",
    "    if airport.get('type'):\n",
    "        obj.type = airport.get('type')\n",
    "    if airport.get('source'):\n",
    "        obj.source = airport.get('source')\n",
    "\n",
    "    obj.latitude = airport.get('latitude')\n",
    "    obj.longitude = airport.get('longitude')\n",
    "\n",
    "    return obj\n",
    "\n",
    "\n",
    "def _airline_to_proto_obj(airline):\n",
    "    \n",
    "    # This part was so frustrating\n",
    "    obj = routes_pb2.Airline()\n",
    "    ## TODO: Create an Airline obj using Protocol Buffers API\n",
    "    if airline is None:\n",
    "        # returning None was throwing a type error\n",
    "        # return None\n",
    "        return obj \n",
    "    if airline.get('airline_id') is None:\n",
    "        # return None\n",
    "        return obj\n",
    "    \n",
    "    # Pull some values from the json\n",
    "    obj.airline_id = airline.get('airline_id')\n",
    "    if airline.get('name'):\n",
    "        obj.name = airline.get('name')\n",
    "    if airline.get('alias'):\n",
    "        obj.alias = airline.get('alias')\n",
    "    if airline.get('iata'):\n",
    "        obj.iata = airline.get('iata')\n",
    "    if airline.get('icao'):\n",
    "        obj.icao = airline.get('icao')\n",
    "    if airline.get('callsign'):\n",
    "        obj.callsign = airline.get('callsign')\n",
    "    if airline.get('country'):\n",
    "        obj.country = airline.get('country')\n",
    "    #if airline.get('active'):\n",
    "    \n",
    "    obj.active = airline.get('active')\n",
    "        \n",
    "    return obj\n",
    "\n",
    "\n",
    "def create_protobuf_dataset(records):\n",
    "    routes = routes_pb2.Routes()\n",
    "    for record in records:\n",
    "        route = routes_pb2.Route()\n",
    "        ## TODO: Implement the code to create the Protocol Buffers Dataset\n",
    "        src_airport = _airport_to_proto_obj(record.get('src_airport'))\n",
    "        dst_airport = _airport_to_proto_obj(record.get('dst_airport'))\n",
    "        airline = _airline_to_proto_obj(record.get('airline'))\n",
    "        codeshare = record.get('codeshare')\n",
    "        equipment = record.get('equipment')\n",
    "        if not src_airport == None:\n",
    "            route.src_airport.CopyFrom(src_airport)\n",
    "        else:\n",
    "            # Required values\n",
    "            route.src_airport.airport_id = 0\n",
    "            route.src_airport.latitude = 0\n",
    "            route.src_airport.longitude = 0\n",
    "            \n",
    "        if not dst_airport == None:\n",
    "            route.dst_airport.CopyFrom(dst_airport)\n",
    "        else:\n",
    "            # Required values\n",
    "            route.dst_airport.airport_id = 0\n",
    "            route.dst_airport.latitude = 0\n",
    "            route.dst_airport.longitude = 0\n",
    "            \n",
    "        if not airline == None:\n",
    "            route.airline.CopyFrom(airline)\n",
    "        else:\n",
    "            # Required values\n",
    "            route.airline.airline_id = 0\n",
    "            route.airline.name = None\n",
    "            route.airline.active = False\n",
    "\n",
    "        route.codeshare = codeshare\n",
    "        route.equipment.extend(equipment)\n",
    "        \n",
    "        routes.route.append(route)\n",
    "\n",
    "    data_path = results_dir.joinpath('routes.pb')\n",
    "\n",
    "    with open(data_path, 'wb') as f:\n",
    "        f.write(routes.SerializeToString())\n",
    "        \n",
    "    compressed_path = results_dir.joinpath('routes.pb.snappy')\n",
    "    \n",
    "    with open(compressed_path, 'wb') as f:\n",
    "        f.write(snappy.compress(routes.SerializeToString()))\n",
    "        \n",
    "create_protobuf_dataset(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.a Simple Geohash Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hash_dirs(records):\n",
    "    geoindex_dir = results_dir.joinpath('geoindex')\n",
    "    geoindex_dir.mkdir(exist_ok=True, parents=True)\n",
    "    hashes = []\n",
    "    ## TODO: Create hash index\n",
    "    for record in records:\n",
    "        try:\n",
    "            src_lat = record.get('src_airport').get('latitude')\n",
    "            src_long = record.get('src_airport').get('longitude')\n",
    "        except:\n",
    "            # Lat and long can't be none, so \"null island\" it is\n",
    "            src_lat = 0\n",
    "            src_long = 0\n",
    "            \n",
    "        location_hash = pygeohash.encode(src_lat, src_long)\n",
    "        hashes.append(dict(hash = location_hash,record = record))\n",
    "\n",
    "    for hashed_location in hashes:\n",
    "        geo_hash = hashed_location.get('hash')\n",
    "        dir_name = results_dir.joinpath('geoindex').joinpath(geo_hash[0]).joinpath(geo_hash[1])\n",
    "        file_name = geo_hash[0:3] + '.jsonl'\n",
    "        path_name = os.path.join(dir_name, file_name)\n",
    "        if not os.path.exists(os.path.dirname(path_name)):\n",
    "            try:\n",
    "                # Make the directory if not exists\n",
    "                os.makedirs(os.path.dirname(path_name))\n",
    "            except OSError as exc: # Guard against race condition\n",
    "                if exc.errno != errno.EEXIST:\n",
    "                    raise\n",
    "        with open(path_name, \"a\") as f: # Append mode\n",
    "            # save the record to a .jsonl file. \n",
    "            f.write(json.dumps(hashed_location.get('record')) + '\\n')\n",
    "            \n",
    "    # Zip up all the .jsonl files we just made\n",
    "    # It is probably more efficient to write directly to .gz files as bytes, but this works too\n",
    "    for root, dirs, files in os.walk(geoindex_dir):\n",
    "        for file in files:\n",
    "            fname = os.path.join(root,file)\n",
    "            with open(fname, 'rb') as f_in:\n",
    "                with gzip.open(fname + '.gz', 'wb') as f_out:\n",
    "                    # Make a zipped copy\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "            # Delete original .jsonl file\n",
    "            os.remove(fname)\n",
    "        \n",
    "            \n",
    "create_hash_dirs(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.b Simple Search Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"airport_id\": 3454, \"name\": \"Eppley Airfield\", \"city\": \"Omaha\", \"country\": \"United States\", \"iata\": \"OMA\", \"icao\": \"KOMA\", \"latitude\": 41.3032, \"longitude\": -95.89409599999999, \"altitude\": 984, \"timezone\": -6.0, \"dst\": \"A\", \"tz_id\": \"America/Chicago\", \"type\": \"airport\", \"source\": \"OurAirports\"}\n"
     ]
    }
   ],
   "source": [
    "def airport_search(latitude, longitude):\n",
    "    \n",
    "    ## TODO: Create simple search to return nearest airport\n",
    "    search_hash = pygeohash.encode(latitude, longitude)\n",
    "    search_dir = results_dir.joinpath('geoindex').joinpath(search_hash[0]).joinpath(search_hash[1])\n",
    "    search_file = search_hash[0:3] + '.jsonl.gz'\n",
    "    search_path = os.path.join(search_dir, search_file)\n",
    "    \n",
    "    # No error checking to see if the file does not exist\n",
    "    # In this case it does, so lets keep it \"simple\"\n",
    "    with open(search_path, 'rb') as f_gz:\n",
    "        with gzip.open(f_gz, 'rb') as f:\n",
    "            closest_airport_distance = math.inf\n",
    "            for line in f:\n",
    "                route = json.loads(line.decode(\"utf-8\"))\n",
    "                lookup_lat = route['src_airport']['latitude']\n",
    "                lookup_long = route['src_airport']['longitude']\n",
    "                # How far apart are the two geohashes?\n",
    "                dist = pygeohash.geohash_approximate_distance(search_hash, pygeohash.encode(lookup_lat,lookup_long))\n",
    "                # Save the closest for later, if we have a new closest\n",
    "                if dist < closest_airport_distance:\n",
    "                    closest_airport = route\n",
    "    # A print statement works here, but a return statement would work better\n",
    "    print(json.dumps(closest_airport['src_airport']))\n",
    "            \n",
    "        \n",
    "airport_search(41.1499988, -95.91779)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
